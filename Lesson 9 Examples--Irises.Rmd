---
title: "Lesson 9 Examples--Irises"
author: "Abra Brisbin"
date: "7/25/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE}
library(dplyr)
library(ggformula)
library(kernlab)
library(caret)
```

## Iris data set
```{r}
summary(iris)
```


Subset to focus on 2 species
```{r}
my_iris <- iris %>%
  filter(Species != "setosa") %>%
  mutate(Species = factor(Species, 
                          levels = c("versicolor",
                                     "virginica")))
```








```{r}
my_iris %>%
  gf_point(Petal.Length ~ Sepal.Length, 
           color =~ Species,
           pch =~ Species)
```


- Data aren't separable -> Maximal margin classifier won't work.
- Division is roughly linear -> Can use a support vector classifier (linear kernel).

```{r}
#library(kernlab)

set.seed(524)
data_used = my_iris

ctrl = trainControl(method = "cv", number = 10)
fit_iris = train(Species ~ Sepal.Length + Petal.Length,
               data = data_used,
               method = "svmLinear",
               tuneGrid = expand.grid(C = c(.001, .01, .1, 1, 5, 10, 100)), # Costs to compare.  Decimals are OK.
               preProcess = c("center","scale"),
               trControl = ctrl)

fit_iris
```


```{r}
fit_iris$finalModel
```






```{r}
attr(fit_iris$finalModel, "SVindex")
```







## Make a graph highlighting the support vectors:
```{r}
my_iris <- my_iris %>%
  tibble::rownames_to_column("Row") %>%
  mutate(is_SV = Row %in% attr(fit_iris$finalModel, "SVindex"))
```








```{r}
my_iris %>%
  filter(is_SV) %>%
  gf_point(Petal.Length ~ Sepal.Length,
             pch =~Species, size = 2.5) %>%
  gf_point(Petal.Length ~ Sepal.Length, 
           color =~ Species,
           pch =~ Species, data = my_iris) %>%
  gf_labs(title = "Highlighted points are support vectors")

```



## Plot the Support Vector Classifier
```{r}
my_iris <- my_iris %>%
  mutate(scale_Petal.Length = scale(Petal.Length),
         scale_Sepal.Length = scale(Sepal.Length))
```




```{r}
b = attr(fit_iris$finalModel, "b")
b # -beta_0
```



```{r}
coefs = attr(fit_iris$finalModel, "coef")[[1]]
head(coefs)
```




```{r}
iris_SV <- my_iris %>%
  filter(is_SV) %>%
  select(c(scale_Sepal.Length, scale_Petal.Length)) %>%
  as.matrix()

head(iris_SV) # Columns should be in the order x, y
              # relative to the graph
```






```{r}
w = colSums(coefs * iris_SV) # beta_1, ... beta_p
w
```









Support Vector Classifier is
$$ \beta_0 + w_1\cdot x_1 + w_2\cdot x_2 = 0 $$

$$ x_2 = -\frac{w_1}{w_2} x_1 - \frac{\beta_0}{w_2} $$
$$ x_2 = -\frac{w_1}{w_2} x_1 + \frac{b}{w_2} $$
```{r}
my_iris %>%
  gf_point(scale_Petal.Length ~ scale_Sepal.Length, 
           color =~ Species,
           pch =~ Species, data = my_iris) %>%
  gf_abline(intercept = b/w[2], slope = -w[1]/w[2]) %>%
  gf_abline(intercept = (b+1)/w[2], slope = -w[1]/w[2], lty = 2) %>%
  gf_abline(intercept = (b-1)/w[2], slope = -w[1]/w[2], lty = 2)
```





## Graphing the unscaled data
```{r}
# Convert the slope and y-intercept to the units of the unscaled data
x = my_iris$Sepal.Length
y = my_iris$Petal.Length

sd.ratio = sd(y)/sd(x)
yint = (sd(y) * b/w[2]) + 
  (w[1]/w[2] * mean(x) * sd.ratio) + mean(y)
my_slope = sd.ratio * -w[1]/w[2]

# Find the y-intercepts of the margin lines, in the units of the unscaled data
yint_upper = (sd(y) * (b+1)/w[2]) + 
  (w[1]/w[2] * mean(x) * sd.ratio) + mean(y)
yint_lower = (sd(y) * (b-1)/w[2]) + 
  (w[1]/w[2] * mean(x) * sd.ratio) + mean(y)
```



```{r}
my_iris %>%
  gf_point(Petal.Length ~ Sepal.Length, 
           color =~ Species,
           pch =~ Species) %>%
  gf_abline(intercept = yint, slope = my_slope) %>%
  gf_abline(intercept = yint_lower, slope = my_slope, 
            lty = 2) %>%
  gf_abline(intercept = yint_upper, slope = my_slope, 
            lty = 2)
```






## Predictions
```{r}
head(predict(fit_iris))
```









```{r}
conf_mat = table(predict(fit_iris), my_iris$Species)
conf_mat
```




```{r}
# Accuracy
sum(diag(conf_mat)) / dim(my_iris)[1]  # (48+46) / 100
```



Recall from `caret`:
```{r}
fit_iris$results[3, ]
```


Why doesn't the confusion matrix give us 0.93?






- Confusion matrix uses the predictions about the full data set, using a model built on the full data set.
- Expect this to be higher than the accuracy on new data.
- `caret` reports the cross-validation accuracy (on new data).


