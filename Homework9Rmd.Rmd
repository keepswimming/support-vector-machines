---
title: "Homework 9 R Markdown"
author: "Rita Miller"
date: '`r Sys.Date()`'
output:
  word_document:
    fig_height: 4
    fig_width: 4.5
  pdf_document:
    fig_height: 4
    fig_width: 4.5
  html_document:
    fig_height: 4
    fig_width: 4.5
---
```{r, setup, include=FALSE}
# Some customization.  You can alter or delete as desired (if you know what you are doing).
knitr::opts_chunk$set(
  echo = TRUE,
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```
Load packages here.
```{r, message=FALSE}
library(readr)
library(dplyr)
library(ggformula)
library(kernlab) #to fit support vector classifiers (SVC) and support vector machines (SVM)
library(caret)
library(ISLR)
```
#### Intellectual Property:
These problems are the intellectual property of the instructors and may not be reproduced outside of this course.

## Problem 1: Using Support Vectors for Classification

In this problem, you will use a support vector classifier to categorize different species of oak trees to different regions (California vs. the Atlantic coast). 

**Data Set**: Download the *Oak_log.csv* data set and load it into R.
```{r}
oak = read.csv("Oak_log.csv")
#head(oak)
#str(oak)
#summary(oak)
```
### Question 1 (2 points): 

Make new columns in the data containing the standardized log(acorn size) and standardized log(range).  

- Note that logSize refers to the log(acorn size).

Make a scatterplot that shows the region, standardized log(acorn size), and standardized log(range) all in the same plot.  Include a legend.  

Use *Insert -> Image* to upload your plot to this question on Canvas.

**Graph Answer**:

#graph data to check whether a support vector classifier (SVC) is likely a good model.
```{r}
oak = oak %>%
  mutate(logRange = scale(logRange), #standardized logRange
         logSize = scale(logSize)) #x-axis
oak %>%
  gf_point(logRange ~ logSize,
           color =~ Region, pch =~ Region, title = "Standardized Acorn Size and Standardized Range")
```

### Question 2 (2 points): #done

Which method do you expect will be better for categorizing the regions of species of oak trees: A support vector classifier or logistic regression? Explain.

**Text Answer**: 
Support Vector Classifier (SVC) may be better for categorizing the regions of species of oak trees. Support vector machines and logistic regression often perform similarly. However, support vector machines frequently perform better when there is little overlap between the categories. There is little overlapping between classes here, but a SVC uses the concept of a "soft margin" and therefore acceptable for some points to be on the other side of the margin/misclassified.

### Question 3 (2 points): 

Use `caret` to build a support vector classifier (a SVM with a linear kernel) to categorize the trees' regions based on their (unstandardized/unscaled) logSize and logRange.  Test the following values of cost:  .001, .01, .1, 1, 5, 10, 100.  Because of this data set's small size, use leave-one-out cross-validation.
Enter your R code below. 

**Code Answer**: 
```{r, question_3}
set.seed(999)
data_used = oak

ctrl = trainControl(method = "LOOCV") 
fit_oak = train(Region ~ logSize + logRange,
                data = data_used, 
                method = "svmLinear",
                tuneGrid = expand.grid(C = c(.001, .01, .1, 1, 5, 10, 100)),
                preProcess = c("center", "scale"),
                prob.model = TRUE, 
                trControl = ctrl)
```
```{r}
#view summary of LOOCV
print(fit_oak)
```
```{r}
#fit_oak$finalModel
fit_oak$bestTune #the best final model is located at row 4
```

### Question 4 (1 point): Double check --->  1e+00
What cost value gave the optimal model?----> 1

**Multiple choice Answer (AUTOGRADED on Canvas)**:  One of .001, .01, .1, 1, 5, 10, 100.

### Question 5 (2 points):

Predict the regions for the entire data set using the final model from `caret`, and make a confusion matrix.  How many Atlantic species are incorrectly classified as California species?

**Numeric Answer (AUTOGRADED on Canvas)**:  2
```{r}
preds = predict(fit_oak)
conf_mat = table(preds, oak$Region)
conf_mat #incorrectly classified, looking for False Positive
#accuracy of model
#sum(diag(conf_mat))/dim(oak)[1]#not required here
```
### Question 6 (1 point):
How many California species are incorrectly classified as Atlantic species?

**Numeric Answer (AUTOGRADED on Canvas)**:  --->0

### Question 7 (3 points):#incomplete

Make a graph showing the data points (similar to question 1), optimal hyperplane (line), and its margins.  Include a legend.

- Use different colors and/or plotting characters to show the Regions of the points.
- You may use either the standardized or the unstandardized log(acorn size) and log(range).

7. Make a graph showing the data points (similar to question 1), optimal hyperplane (line), and its margins.  
Include a legend. 
```{r}
oak <- oak %>%
 mutate(is_SV = Row %in% attr(fit_oak$finalModel, "SVindex"))

b = attr(fit_oak$finalModel, "b")
coefs = attr(fit_oak$finalModel, "coef")[[1]]

oak_SV <- oak %>%
 filter(is_SV) %>%
 select(c(scale_logSize, scale_logRange)) %>%
 as.matrix()
#head(oak_SV) # Columns should be in the order x, y
 # relative to the graph

w = colSums(coefs * oak_SV) # beta_1, ... beta_p

oak %>%
 gf_point(scale_logRange ~ scale_logSize,
 color =~ Region,
 pch =~ Region) %>%
 gf_abline(intercept = b/w[2], slope = -w[1]/w[2]) %>%
 gf_abline(intercept = (b+1)/w[2], slope = -w[1]/w[2], lty = 2) %>%
 gf_abline(intercept = (b-1)/w[2], slope = -w[1]/w[2], lty = 2)
```

### Question 8 (4 points)
In question 3, we used `caret` to perform cross-validation for model selection (picking the best value of the cost parameter).  In this question, we will use a *for* loop wrapper to perform a second layer of cross-validation.  This will allow us to honestly assess the accuracy of our model-selection process.

- Set the random seed to 9.
- Create a vector to store the predicted regions (it should have length = the number of rows in the data set).
- Create vectors `groups` and `cv_groups` to perform 10-fold CV (like we did before learning about `caret`).
- Create a *for* loop to iterate through the folds of the outer layer of CV.  Inside the *for* loop:
  - Create the variables `groupii`, `train_set`, and `test_set`, like we did for CV before learning about `caret`.
  - Use `caret` to perform an inner layer of LOOCV.  `caret` should fit a support vector classifier and choose among costs of .001, .01, .1, 1, 5, 10, 100.  This code can be the same as you used in question 3, **except that it should use the training set from the outer layer of CV** instead of the entire data set.
  - Use the model from `caret` to predict the regions of the data in the test set from the outer layer of CV.
  
Enter your R code below. 

**Code Answer**: 

```{r}
set.seed(9)
data_used = oak
n = dim(oak)[1] 

ngroups = 10 # 10-fold outer CV
groups = rep(1:ngroups, length = n) 

cv_groups = sample(groups, n)

preds = factor(levels = c("Atlantic", "California")) 

ctrl = trainControl(method = "LOOCV")

for(ii in 1:ngroups){
  groupii = (cv_groups == ii)
  train_set = oak[!groupii, ]
  test_set = oak[groupii, ]
  data_used = train_set
  fit_cv = train(Region ~ logSize + logRange,
               data = data_used,
               method = "svmLinear",
               tuneGrid = expand.grid(C = c(.001, .01, .1, 1, 5, 10, 100)),
               preProcess = c("center","scale"),
               #prob.model = TRUE, #used to compute probabilities later
               trControl = ctrl)
    preds[groupii] = predict(fit_cv, newdata = test_set)
}

```
### Question 9 (1 point)
Make a confusion matrix of the stored predictions from the double cross-validation.  What is the cross-validation accuracy of the model selection process?  Enter your answer to 4 decimal places.  

**Numeric Answer (AUTOGRADED on Canvas)**:  
```{r}
fit_cv
```


```{r}
preds = predict(fit_oak)
conf_mat = table(preds, oak$Region)
conf_mat 

#cv accuracy of model 
sum(diag(conf_mat))/dim(oak)[1] #0.9487 wrong
```

## Problem 2:  Using Support Vector Machines for Classification

In this problem, you will use a support vector machine with a radial kernel to classify the gas mileage of cars.  
**Data Set**: Load the **Auto** data set, which is in the **ISLR** library.

### Question 10 (2 points): #double check

- **Tell R to treat the origin variable as a factor.** 
- Create a binary variable that equals "high" for cars with gas mileage (`mpg`) greater than or equal to the median and "low" for cars with gas mileage below the median.  
Tell R to treat it as a factor.  
- Remove the `name` variable and the continuous `mpg` variable from the data frame.
 
Enter your R code below. 

**Code Answer**: 
```{r}
#library(ISLR)
data("Auto")
```
```{r}
#Create a binary variable that equals "high" for cars with gas mileage (`mpg`) greater than or equal #to the median and "low" for cars with gas mileage below the median. 
#if mpg >= median mpg, = "high", else = 0
#if mpg < median mpg, = "low", else = 0
Auto$myMpg = as.factor(ifelse(Auto$mpg > median(Auto$mpg), "high", "low"))
#set origin as multi-class factor.
Auto$origin = as.factor(Auto$origin)

data_used = Auto%>%
  select(-c(mpg, name))
```
### Question 11 (3 points):

Set the random seed to 9.  Use `caret` to perform 10-fold cross-validation to compare different values of cost and sigma for a radial support vector machine. Use the same values of cost as listed previously: .001, .01, .1, 1, 5, 10, 100. Use sigma = 0.5, 1, 2, 3, and 4. 

- Model the binary gas mileage variable as a function of all the other variables that are in Auto after question 10.
- Ask `caret` to model the probability that each point belongs to each category.  (For purposes of this homework, it's OK if the model fails to converge.)

Enter your R code below.  


**Code Answer**: 
```{r, results = "hide", question_11}

set.seed(9)
#data_used = Auto%>%
 # select(-c(mpg, name))
  
ctrl = trainControl(method = "cv", number = 10, classProbs = TRUE) 

fit_radial = train(myMpg ~.,
                data = data_used, 
                method = "svmRadial",
                tuneGrid = expand.grid(C = c(.001, .01, .1, 1, 5, 10, 100),
                                        sigma = c(0.5, 1, 2, 3, 4)),
                preProcess = c("center", "scale"),
                prob.model = TRUE, #it's ok if it fails to converge 
                trControl = ctrl)
```

### Questions 12-13  (2 points, 1 each):

Which combination of parameters gave the highest cross-validation accuracy? 
#just add the line which has the highest cost, accuracy.....
```{r}
fit_radial$bestTune
```


```{r}
fit_radial$bestTune
```
```{r}
#when using predict(fit_radial) ur using the $finalModel from caret, which uses the best tuning parameter values, but which was re fit on the whole data set. So the accuracy from the confusion matrix is the accuracy on all data points, when those data points were used to build the model. So it makes sense this accuracy would be higher than the cv accuracy. 
#predict(finalModel)
```

```{r}
#and find the maximum value in the “Accuracy” column of the detailed performance results. Alternatively, #you can extract the maximum accuracy using
fit_radial$results %>%
  filter(sigma == 1 , C == 1) #this is a focus in tool 
```
***(Numeric answers, AUTOGRADED on Canvas)**

Cost: 1  
Sigma:  1 

### Question 14 (1 point):

What was the cross-validation accuracy of the best model?  Enter your answer to 4 decimal places.  

**Numeric Answer (AUTOGRADED on Canvas)**:  0.9310
 
### Question 15 (2 points): 

Use the best model to predict the probability that the following car would have **high** gas mileage:  
**1977 Chrysler Sunbeam**  
**Cylinders**: 4  
**Engine displacement**:  132.5 cubic inches  
**Horsepower**:  155  
**Weight**:  2,910 lbs  
**Acceleration**:  8.3 seconds  
**Origin**:  American (1)  

Enter your answer to 4 decimal places.  

   
**Numeric Answer (AUTOGRADED on Canvas)**:  
```{r}
newCar = data.frame(year = 77,
                    cylinders = 4,
                    displacement = 132.5, 
                    horsepower = 155,
                    weight = 2910, 
                    acceleration = 8.3, 
                    origin = factor(1, levels = c(1,2,3))
                   )
                   
```
```{r}
preds = round(predict(fit_radial, newCar, type="prob"),4) 
preds #high was 0.5286
```

### Question 16 (2 points) #incomplete
Make a grid of example data points with 

* weight = seq(min(Auto$weight), max(Auto$weight), length = 100)
* cylinders = 4
* origin = 1
* all other predictors set equal to their medians.
```{r}

newCar = expand.grid(newCar = seq(-3.4, 1.4, by = .01),
                   duration = seq(0, 3650, by = 4))
newCar = expand.grid(weight = seq(min(Auto$weight), max(Auto$weight), length = 100),
                    cylinders = 4,
                    origin = as.factor(1),
                    displacement=median(Auto$displacement),
                    horsepower=median(Auto$horsepower),
                    acceleration=median(Auto$acceleration),
                    year=median(Auto$year))
                   
newCar
```

Predict the probability of having high gas mileage for each data point in the grid.  Include your R code below.

**Code Answer**:
```{r}
preds = predict(fit_radial, newdata = newCar, type = "prob") #predict prob of high gas mileage
preds$high
```

### Question 17 (2 points) 
Make a graph of the predicted probability of high gas mileage as a function of weight.  

Include a title or caption on the graph that summarizes the relationship between weight and predicted probability of high gas mileage.

Use *Insert -> Image* to upload your plot to this question on Canvas.
```{r}

newCar$prob_high <-  preds[ ,1]

ggplot(newCar, aes(x=weight, y = prob_high)) +
  geom_point() +
  ggtitle("Plotting Relation Weight vs High Mileage") +
  xlab("Weight of Car") +
  ylab("High Mileage Probability")
```
Include a title or caption on the graph that summarizes the relationship between weight and predicted probability of high gas mileage.

Use *Insert -> Image* to upload your plot to this question on Canvas.

 



